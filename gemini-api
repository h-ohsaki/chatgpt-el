#!/usr/bin/env python3
#
# A simple Gemini client using Google Generative AI module.
# Copyright (c) 2025, Hiroyuki Ohsaki.
# All rights reserved.
#

import os
import re
import sys

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from perlcompat import die, warn, getopts

# Model Name             Output   Input    Speed        Context
# -----------------------------------------------------------------
# gemini-3-pro-preview   $12.00   $2.00    Variable*    1M
# gemini-2.5-flash       $0.20    $0.05    Fastest      2M
# gemini-1.5-pro         $10.50   $3.50    Medium       2M
# gemini-1.5-flash       $0.30    $0.075   Fast         2M

INSTRUCTIONS = """\
あなたは親切で信頼できる優秀な AI アドバイザです。
ユーザーの質問意図をくみ取り、
論理的な文章を心がけてください。

読者は情報工学分野の大学教員です。
情報工学や情報ネットワークに関する基本的な事柄は説明する必要がありません。

文章は自然な日本語で、
必要に応じて段落や箇条書き、
表現の工夫 (例や補足) を使って Markdown 形式で説明してください。
不要な繰り返しは避けながら、
有益な情報をできるだけ多く提供してください。

専門用語には元となる英語を括弧書きで表記してください。

事実やエビデンスを重視して回答してください。
最新の研究やデータに基づいた情報を提供してください。
事実に関する質問に答える際、
自身の知識だけで 100% の確信が持てない場合は、
決して推測で答えないでください。
代わりに「情報が確認できなかった」と答えてください。

誤解されることが多い概念には、
どのように誤解されることが多いか例とともに示してください。

もし可能であれば、
より深く学ぶために役立つと思われる、
周辺情報や、エピソード、類似した概念との関係性を説明してください。
"""

def usage():
    die(f"""\
usage: {sys.argv[0]} [-vc] [-e engine] [-m model] [-i instr] [-s prompt]
  -v         verbose mode
  -c         compare mode (also display the reply from gemini-1.5-flash)
  -e engine  specify AI engine (default: gemini)
  -m model   specify LLM model (default: gemini-1.5-pro)
  -i instr   specify instructions
  -s prompt   prompt string
""")

def expand_macros(astr):
    def _include(match):
        file = match.group(1).strip()
        file = file.replace('~', os.getenv('HOME'))
        try:
            with open(file) as f:
                return f.read()
        except FileNotFoundError:
            return match.group(0)

    pattern = re.compile(r'\[\[(.+?)\]\]')
    return pattern.sub(_include, astr)

def send_prompt(model_name,
                instructions,
                prompt,
                max_output_tokens=8192,
                temperature=0.):
    if verbose:
        print(f'[{model_name}]')
    model = genai.GenerativeModel(model_name=model_name,
                                  system_instruction=instructions)
    generation_config = genai.types.GenerationConfig(
        max_output_tokens=max_output_tokens,
        temperature=temperature,
    )
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT:
        HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH:
        HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:
        HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT:
        HarmBlockThreshold.BLOCK_NONE,
    }
    response = model.generate_content(prompt,
                                      generation_config=generation_config,
                                      safety_settings=safety_settings,
                                      stream=True)
    for chunk in response:
        print(chunk.text, end='', flush=True)
    print()

def main():
    global verbose
    opt = getopts('vce:m:i:s:') or usage()
    verbose = opt.v
    compare = opt.c
    engine = opt.e or 'gemini'
    model = opt.m or 'gemini-2.5-flash'
    instructions = opt.i or INSTRUCTIONS
    prompt = opt.s if opt.s else sys.stdin.read().strip()
    prompt = expand_macros(prompt)

    api_key = os.environ.get('GOOGLE_API_KEY')
    genai.configure(api_key=api_key)
    send_prompt(model, instructions, prompt)

    if compare:
        print('-' * 64)
        send_prompt('gemini-2.5-flash', instructions, prompt)

if __name__ == "__main__":
    main()
