#!/usr/bin/env python3
#
# A simple Gemini client using Google Generative AI module.
# Copyright (c) 2025, Hiroyuki Ohsaki.
# All rights reserved.
#

import os
import re
import sys

from google.genai import types
# Note: 'perlcompat' and 'tbdump' appear to be custom or local utility modules.
from perlcompat import die, warn, getopts
import google.genai as genai
import tbdump

# Model Name             Output   Input    Speed        Context
# -----------------------------------------------------------------
# gemini-3-pro-preview   $12.00   $2.00    Variable* 1M
# gemini-2.5-flash       $0.20    $0.05    Fastest      2M
# gemini-1.5-pro         $10.50   $3.50    Medium       2M
# gemini-1.5-flash       $0.30    $0.075   Fast         2M
# imagen-4.0-ultra-generate-001
# imagen-4.0-generate-001
# gemini-3-pro-image-preview

INSTRUCTIONS = """\
あなたは親切で信頼できる優秀な AI アドバイザです。
ユーザーの質問意図をくみ取り、
論理的な文章を心がけてください。

読者は情報工学分野の大学教員です。
情報工学や情報ネットワークに関する基本的な事柄は説明する必要がありません。

文章は自然な日本語で、
必要に応じて段落や箇条書き、
表現の工夫 (例や補足) を使って Markdown 形式で説明してください。
不要な繰り返しは避けながら、
有益な情報をできるだけ多く提供してください。

専門用語には元となる英語を括弧書きで表記してください。

事実やエビデンスを重視して回答してください。
最新の研究やデータに基づいた情報を提供してください。
事実に関する質問に答える際、
自身の知識だけで 100% の確信が持てない場合は、
決して推測で答えないでください。
代わりに「情報が確認できなかった」と答えてください。

誤解されることが多い概念には、
どのように誤解されることが多いか例とともに示してください。

もし可能であれば、
より深く学ぶために役立つと思われる、
周辺情報や、エピソード、類似した概念との関係性を説明してください。
"""

def usage():
    die(f"""\
usage: {sys.argv[0]} [-vlI] [-m model] [-i instr] [-s prompt] [-O path]
  -v         verbose mode
  -l         list available models
  -m model   specify LLM model (default: gemini-1.5-pro)
  -i instr   specify instructions
  -s prompt  prompt string
  -O path    the output filename for generated image
""")

def expand_macros(astr):
    """Replaces patterns like [[filename]] in the prompt with the content of
    the specified file.  Supports tilde expansion (e.g., [[~/file.txt]])."""
    def _include(match):
        file = match.group(1).strip()
        file = file.replace('~', os.getenv('HOME'))
        try:
            with open(file) as f:
                return f.read()
        except FileNotFoundError:
            # If file not found, leave the macro as is.
            return match.group(0)

    pattern = re.compile(r'\[\[(.+?)\]\]')
    return pattern.sub(_include, astr)

def send_prompt(client,
                model_name,
                instructions,
                prompt,
                max_output_tokens=8192,
                temperature=0.):
    if verbose:
        print(f'[{model_name}]')

    # Configure generation parameters including safety settings.
    # We explicitly set all safety thresholds to BLOCK_NONE to minimize filtering.
    config = types.GenerateContentConfig(
        temperature=temperature,
        system_instruction=instructions,
        safety_settings=[
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                threshold=types.HarmBlockThreshold.BLOCK_NONE),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                threshold=types.HarmBlockThreshold.BLOCK_NONE),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                threshold=types.HarmBlockThreshold.BLOCK_NONE),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                threshold=types.HarmBlockThreshold.BLOCK_NONE),
        ])

    # Use streaming to display the response incrementally as it is generated.
    stream = client.models.generate_content_stream(model=model_name,
                                                   contents=prompt,
                                                   config=config)
    for chunk in stream:
        if chunk.text:
            print(chunk.text, end='', flush=True)
    print()

def generate_image(client, model_name, prompt, outfile):
    """Generates an image using dedicated image generation models (e.g.,
    Imagen)."""
    if verbose:
        print(f'[{model_name}]')

    # 'safety_filter_level' is set to the most permissive level allowed via API.
    # 'include_rai_reason' is crucial for debugging if the prompt triggers a safety block.
    config = types.GenerateImagesConfig(number_of_images=1,
                                        safety_filter_level='block_low_and_above',
                                        include_rai_reason=True)
    response = client.models.generate_images(model=model_name,
                                             prompt=prompt,
                                             config=config)
    if response.generated_images:
        image_data = response.generated_images[0].image.image_bytes
        print(f'Writing as {outfile}...')
        with open(outfile, 'wb') as f:
            f.write(image_data)
    else:
        print(f'No images returned from model {model_name}.')

def generate_image_via_gemini(client, model_name, prompt, outfile):
    """Attempts to extract inline images generated by multimodal models (e.g.,
    Gemini) instead of dedicated image models."""
    if verbose:
        print(f'[{model_name}]')

    # Note: Gemini uses a list of SafetySetting objects, unlike Imagen's single parameter.
    safety_settings = [
        types.SafetySetting(category=category, threshold='BLOCK_NONE')
        for category in [
            'HARM_CATEGORY_HATE_SPEECH', 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
            'HARM_CATEGORY_DANGEROUS_CONTENT', 'HARM_CATEGORY_HARASSMENT'
        ]
    ]
    # Use GenerateContentConfig (for LLMs) instead of GenerateImagesConfig.
    config = types.GenerateContentConfig(temperature=.1,
                                         safety_settings=safety_settings)

    response = client.models.generate_content(model=model_name,
                                              contents=prompt,
                                              config=config)

    image_found = False
    # Parse the response to find any inline binary data (images).
    if response.candidates and response.candidates[0].content.parts:
        for part in response.candidates[0].content.parts:
            if part.inline_data:
                print(f'Writing inline image to {outfile}...')
                with open(outfile, 'wb') as f:
                    f.write(part.inline_data.data)
                image_found = True
                break

    if not image_found:
        print(f'No inline image data found in response from {model_name}.')
        if response.text:
            print(f'Response text: {response.text}')

def list_models(client):
    for model in client.models.list():
        # Clean up model name for display (remove 'models/' prefix).
        name = model.name.replace('models/', '')
        disp_name = model.display_name
        methods = ', '.join(model.supported_actions or [])
        print(f'{name}\t{disp_name}\t{methods}')

def main():
    global verbose
    opt = getopts('vlIe:m:i:s:O:') or usage()
    verbose = opt.v
    do_list = opt.l
    gen_image = opt.I
    model = opt.m or 'gemini-2.5-flash'
    instructions = opt.i or INSTRUCTIONS
    prompt = opt.s
    outfile = opt.O or 'out.png'

    api_key = os.environ.get('GOOGLE_API_KEY')
    client = genai.Client(api_key=api_key)
    if do_list:
        list_models(client)
        exit()

    # Read prompt from stdin if not provided via arguments.
    if not prompt:
        prompt = sys.stdin.read().strip()
    prompt = expand_macros(prompt)

    # Dispatch to the appropriate function based on the model type and user intent.
    if gen_image:
        # Check if the model name suggests an Imagen model to use the correct API endpoint.
        if 'imagen' in model:
            generate_image(client, model, prompt, outfile)
        else:
            generate_image_via_gemini(client, model, prompt, outfile)
    else:
        send_prompt(client, model, instructions, prompt)

if __name__ == "__main__":
    main()
