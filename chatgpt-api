#!/usr/bin/env python3
#
# A simple ChatGPT/Gemini/Ollama client using API.
# Copyright (c) 2025, Hiroyuki Ohsaki.
# All rights reserved.
#

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import collections
import json
import os
import re
import requests
import sys

from perlcompat import die, warn, getopts
import google.genai
import openai
import tbdump

OLLAMA_URL = 'http://vcserv:11434/api/generate'
OLLAMA_MODEL = 'ministral-3:14b'

INSTRUCTIONS = """\
あなたは親切で信頼できる優秀な AI アドバイザです。
ユーザーの質問意図をくみ取り、
論理的な文章を心がけてください。

読者は情報工学分野の大学教員です。
情報工学や情報ネットワークに関する基本的な事柄は説明する必要がありません。

文章は自然な日本語で、
必要に応じて段落や箇条書き、
表現の工夫 (例や補足) を使って Markdown 形式で説明してください。
不要な繰り返しは避けながら、
有益な情報をできるだけ多く提供してください。

専門用語には元となる英語を括弧書きで表記してください。

事実やエビデンスを重視して回答してください。
最新の研究やデータに基づいた情報を提供してください。
事実に関する質問に答える際、
自身の知識だけで 100% の確信が持てない場合は、
決して推測で答えないでください。
代わりに「情報が確認できなかった」と答えてください。

誤解されることが多い概念には、
どのように誤解されることが多いか例とともに示してください。

もし可能であれば、
より深く学ぶために役立つと思われる、
周辺情報や、エピソード、類似した概念との関係性を説明してください。
"""

def usage():
    die(f"""\
usage: {sys.argv[0]} [-vl] [-e engine] [-m model] [-i inst] [-s prompt] [-O path]
  -v         verbose mode
  -l         list available modes
  -e engine  specify engine (default: ollama)
  -m model   specify LLM model (default: qwen-2.5:14b)
  -i inst    specify system instructions
  -s prompt  prompt string
  -O path    output image filename
""")

# Ollama ----------------------------------------------------------------
class Ollama:
    def __init__(self, model=None, inst='', max_tokens=8192, temp=.1):
        if model is None:
            model = OLLAMA_MODEL
        self.model = model
        self.inst = inst
        self.max_tokens = max_tokens
        self.temp = temp
        self._line_counts = collections.defaultdict(int)
        self._line_buf = ''
        self._output_size = 0

    def expand_macros(self, astr):
        def _include(match):
            file = match.group(1).strip()
            file = file.replace('~', os.getenv('HOME'))
            try:
                with open(file) as f:
                    return f.read()
            except FileNotFoundError:
                return match.group(0)

        pattern = re.compile(r'\[\[(.+?)\]\]')
        return pattern.sub(_include, astr)

    def list_models(self):
        print("""\
deepseek-coder:6.7b
deepseek-coder-v2:16b
deepseek-r1:14b
deepseek-r1:14b-jpn
deepseek-r1:1.5b
deepseek-r1:7b
deepseek-r1:8b
gemma3:12b-it-qat
gemma3:12b-it-qat-jp
gemma3:1b-it-qat
gemma3:27b-it-qat
gemma3:4b-it-qat
ministral-3:3b-instruct-2512-q4_K_M
ministral-3:8b-it-q4_k_m
mistral:7b-instruct
phi4:14b
phi4:15b-q3_K_M
qwen2.5:3b
qwen2.5:7b
qwen2.5-coder:14b
qwen2.5-coder:7b
qwen3:14b
qwen3:4b
qwen3:8b
""")

    def _print(self, buf):
        sys.stdout.write(buf)
        sys.stdout.flush()
        self._line_buf += buf
        self._output_size += len(buf)
        if self._output_size > 20000:
            raise RuntimeError(f'Too long output: {self._output_size}')
        while '\n' in self._line_buf:
            line, rest = self._line_buf.split('\n', 1)
            self._line_buf = rest
            if len(line) >= 16:
                self._line_counts[line] += 1
                if self._line_counts[line] > 20:
                    raise RuntimeError(f'Too many repeate lines: {line}')

    def send_prompt(self, prompt, outfile):
        prompt = self.expand_macros(prompt)
        payload = {
            'model': self.model, 'system': self.inst, 'prompt': prompt,
            'stream': True, 'num_predict': self.max_tokens, 'options':
            {'temperature': self.temp, 'num_ctx': 4096 * 2}
        }
        response = requests.post(OLLAMA_URL,
                                 json=payload,
                                 stream=True,
                                 headers={'Content-Type': 'application/json'})
        try:
            response.raise_for_status()
        except Exception as e:
            die(f'{e}')

        print(f'[{self.model}]', flush=True)
        for line in response.iter_lines(decode_unicode=True):
            if not line:
                continue
            line = line.replace(b'\\n', b'__NEWLINE__')
            data = json.loads(line)
            if 'response' in data:
                token = data['response']
                token = token.replace('__NEWLINE__', '\n')
                self._print(token)
        print()

# ChatGPT ----------------------------------------------------------------
class ChatGPT(Ollama):
    def __init__(self, model=None, *args, **kwargs):
        if model is None:
            model = 'gpt-4.1-mini'
        super().__init__(model=model, *args, **kwargs)
        self.client = openai.OpenAI()

    def list_models(self):
        print("""\
model          output  intel  speed  cutoff
gpt-4.1-nano   $0.4    2      5      2024-06-01
gpt-4o-mini    $0.6    2      4      2023-10-01
gpt-4.1-mini   $1.6    3      4      2024-06-01
gpt-5-nano     $0.4    2      5      2024-05-31
gpt-5-mini     $2      3      4      2024-05-31
o1-mini        $4.4    R3     2      2023-10-01
o3-mini        $4.4    R4     3      2024-10-01
o4-mini        $4.4    R4     3      2024-06-01
o3             $8      R5     1      2024-06-01
gpt-4o         $10     3      3      2023-10-01
gpt-5.1        $10     4      4      2025-04-01
gpt-5.2	       $14     4      4      2025-08-31
gpt-4-turbo    $30     2      3      2023-12-01
o1             $60     R4     1      2023-10-01
gpt-5.2-pro    $168    4      1      2025-08-31
""")

    def send_prompt(self, prompt, outfile):
        prompt = self.expand_macros(prompt)
        params = dict(
            model=self.model,
            instructions=self.inst,
            input=prompt,
            stream=True,
            max_output_tokens=self.max_tokens,
        )
        if not self.model.startswith(('gpt-5', 'o')):
            params["temperature"] = self.temp
        try:
            stream = self.client.responses.create(**params)
        except Exception as e:
            die(f'{e}')

        print(f'[{self.model}]')
        for event in stream:
            try:
                delta = event.delta
                self._print(delta)
            except AttributeError:
                pass
        print()

# Gemini ----------------------------------------------------------------
class Gemini(Ollama):
    def __init__(self, model=None, *args, **kwargs):
        if model is None:
            model = 'gemini-flash-preview'
        super().__init__(model=model, *args, **kwargs)
        api_key = os.environ.get('GOOGLE_API_KEY')
        self.client = google.genai.Client(api_key=api_key)

    def list_all_models(self):
        for model in self.client.models.list():
            # Clean up model name for display (remove 'models/' prefix).
            name = model.name.replace('models/', '')
            disp_name = model.display_name
            methods = ', '.join(model.supported_actions or [])
            print(f'{name}\t{disp_name}\t{methods}')

    def list_models(self):
        print("""\
model                          output   input    speed        context
gemini-2.5-flash               $0.2     $0.05    Fastest      2M
gemini-1.5-flash               $0.3     $0.075   Fast         2M
gemini-3-flash-preview         $3       $0.5
gemini-1.5-pro                 $10.5    $3.5     Medium       2M
gemini-3-pro-preview           $12      $2       Variable*    1M
imagen-4.0-generate-001        $0.04    n/a      n/a          n/a
imagen-4.0-ultra-generate-001  $0.06    n/a      n/a          n/a
gemini-3-pro-image-preview     $0.134   n/a      n/a          n/a""")

    def _safety_settings(self):
        setting = google.genai.types.SafetySetting
        category = google.genai.types.HarmCategory
        thresh_none = google.genai.types.HarmBlockThreshold.BLOCK_NONE
        safety_settings = [
            setting(category=category.HARM_CATEGORY_HARASSMENT,
                    threshold=thresh_none),
            setting(category=category.HARM_CATEGORY_HATE_SPEECH,
                    threshold=thresh_none),
            setting(category=category.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                    threshold=thresh_none),
            setting(category=category.HARM_CATEGORY_DANGEROUS_CONTENT,
                    threshold=thresh_none),
        ]
        return safety_settings

    def _send_prompt(self, prompt, outfile):
        # Configure generation parameters including safety settings.
        # We explicitly set all safety thresholds to BLOCK_NONE to minimize filtering.
        config = google.genai.types.GenerateContentConfig(
            temperature=self.temp,
            system_instruction=self.inst,
            safety_settings=self._safety_settings())

        # Use streaming to display the response incrementally as it is generated.
        try:
            stream = self.client.models.generate_content_stream(
                model=self.model, contents=prompt, config=config)
            print(f'[{self.model}]')
            for chunk in stream:
                if chunk.text:
                    self._print(chunk.text)
            print()
        except Exception as e:
            die(f'{e}')

    def _generate_image_imagen(self, prompt, outfile):
        config = google.genai.types.GenerateImagesConfig(
            number_of_images=1,
            safety_filter_level='block_low_and_above',
            include_rai_reason=True)
        response = self.client.models.generate_images(model=self.model,
                                                      prompt=prompt,
                                                      config=config)
        if response.generated_images:
            image_data = response.generated_images[0].image.image_bytes
            print(f'Writing image to {outfile}...')
            with open(outfile, 'wb') as f:
                f.write(image_data)
        else:
            print(f'No images returned from model {self.model}.')
            if response.text:
                print(f'Response text: {response.text}')

    def _generate_image_gemini(self, prompt, outfile):
        config = google.genai.types.GenerateContentConfig(
            temperature=self.temp, safety_settings=self._safety_settings())
        response = self.client.models.generate_content(model=self.model,
                                                       contents=prompt,
                                                       config=config)

        image_found = False
        # Parse the response to find any inline binary data (images).
        if response.candidates and response.candidates[0].content.parts:
            for part in response.candidates[0].content.parts:
                if part.inline_data:
                    print(f'Writing inline image to {outfile}...')
                    with open(outfile, 'wb') as f:
                        f.write(part.inline_data.data)
                    image_found = True
                    break

        if not image_found:
            print(f'No inline image returned from model {self.model}.')
            if response.text:
                print(f'Response text: {response.text}')

    def send_prompt(self, prompt, outfile):
        prompt = self.expand_macros(prompt)
        if 'imagen' in self.model:
            self._generate_image_imagen(prompt, outfile)
        elif 'image' in self.model:
            self._generate_image_gemini(prompt, outfile)
        else:
            self._send_prompt(prompt, outfile)

ENGINES = {'ollama': Ollama, 'chatgpt': ChatGPT, 'gemini': Gemini}

def main():
    opt = getopts('vle:m:i:s:O:') or usage()
    verbose = opt.v
    do_list = opt.l
    engine = opt.e.lower() if opt.e else 'ollama'
    model = opt.m
    inst = opt.i or INSTRUCTIONS
    prompt = opt.s
    outfile = opt.O if opt.O else 'out.png'

    cls_ = ENGINES[engine]
    client = cls_(model=model, inst=inst)
    if do_list:
        client.list_models()
    else:
        if not prompt:
            prompt = sys.stdin.read().strip()
        client.send_prompt(prompt, outfile)

if __name__ == "__main__":
    main()
